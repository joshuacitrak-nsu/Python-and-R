---
title: "bayesian regression modeling"
author: "Matt Rosinski"
date: "2023-03-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Future exploration - Youtube API
```{r}
# https://github.com/soodoku/tuber

```


# Spotifyr Documentation
```{r}
# devtools::install_github('charlie86/spotifyr')

# Documentation
# https://www.rcharlie.com/spotifyr/

```

# Spotifyr documentation
https://www.rdocumentation.org/packages/spotifyr/versions/2.2.4

# Load Libraries
```{r}
library(tidyverse)
library(spotifyr)
library(rstanarm)
library(loo)
library(tidybayes)
library(broom)
library(broom.mixed)
```


```{r}
Sys.setenv(SPOTIFY_CLIENT_ID = 'getyourown')
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'gotospotifywebapi')
Sys.setenv(SPOTIFY_REDIRECT_URI = "http://localhost:1410/")

access_token <- get_spotify_access_token()
```

# Find the Spotify ID for an artist
```{r}
# Replace this with the name of the artist you are looking for
search_string <- "Radiohead"

search_results <- search_spotify(
  q = search_string,
  type = c("artist"),
  authorization = get_spotify_access_token()
)

artist_id <- search_results %>% 
    arrange(desc(popularity)) %>% 
    select(id, name, uri, popularity, followers.total) %>% 
    slice_head(n = 1) %>% 
    select(id) %>% 
    pull()

artist_id
```
# Get artist audio features
```{r}
search_string <- "Radiohead"

radiohead_audio_dt <- 
    get_artist_audio_features(
    artist = artist_id,
    include_groups = "album",
    return_closest_artist = TRUE,
    dedupe_albums = TRUE,
    market = NULL,
    authorization = get_spotify_access_token()
    )

radiohead_audio_dt %>% glimpse()

```
# Get track popularity scores
```{r}

get_multiple_tracks <- function(track_ids, limit = 50, time_interval = 2, authorization) {
  # Calculate the number of chunks required
  num_chunks <- ceiling(length(track_ids) / limit)
  
  # Split the track_ids into chunks
  track_id_chunks <- split(track_ids, ceiling(seq_along(track_ids) / limit))
  
  # Function to get tracks for a chunk and wait for the specified time interval
  get_tracks_chunk <- function(chunk, time_interval, authorization) {
    track_info <- get_tracks(
      ids = chunk,
      market = NULL,
      authorization = authorization,
      include_meta_info = FALSE
    )
    Sys.sleep(time_interval)
    return(track_info)
  }
  
  # Get track information for each chunk and combine the results
  combined_results <- map_dfr(track_id_chunks, get_tracks_chunk, time_interval, authorization)
  
  return(combined_results)
}

# Usage
access_token <- get_spotify_access_token()
track_ids <- radiohead_audio_dt %>% select(track_id) %>% pull()
combined_results <- get_multiple_tracks(track_ids, limit = 50, time_interval = 2, authorization = access_token)

popularity_score_df <- 
  combined_results %>% 
  select(id, name, popularity, uri, album.name, album.release_date)

popularity_score_df

```

# Combine the two data frames
```{r}
names(popularity_score_df)
names(radiohead_audio_dt)

songs <- 
    radiohead_audio_dt %>% 
    inner_join(popularity_score_df, by = c("track_name" = "name")) %>% 
    select(track_name, 
           artist_name, 
           album_release_date, 
           danceability:tempo, 
           time_signature, 
           duration_ms,
           popularity) %>% 
    group_by(track_name) %>%
    slice_max(popularity, n = 1) %>%
    ungroup() %>%
    # distinct(track_name, .keep_all = TRUE) %>%  # Keep only the first occurrence
    mutate(
      album_release_date = parse_date_time(album_release_date, c("ymd", "Y")),
      song_age = as.numeric(Sys.Date() - as.Date(album_release_date)))

songs

write_csv(songs, "data/radiohead_songs.csv")


```

# Bayesian regression

## Install rstanarm
```{r}
# I have 8 cores and am using 4 cores for install
# I tried installing the development version of rstanarm with
# rstan package and C++ toolchain using the instructions here: https://github.com/stan-dev/rstanarm
# 
# Sys.setenv(MAKEFLAGS = "-j4")
# Sys.setenv("R_REMOTES_NO_ERRORS_FROM_WARNINGS" = "true")
# remotes::install_github("stan-dev/rstanarm", INSTALL_opts = "--no-multiarch", force = TRUE)
# but got this warning:
# Warning: installation of package ‘C:/Users/mattr/AppData/Local/Temp/RtmpmMWMST/file56301dda3f47/rstanarm_2.21.3.tar.gz’
# had non-zero exit status

# So used version released on CRAN
# install.packages("rstanarm")

```
## Check RStan installation
```{r}

# example(stan_model, package = "rstan", run.dontrun = TRUE)
```

## Import data
```{r}
songs <- read_csv("data/radiohead_songs.csv")
songs %>% glimpse()

```

## Frequentist linear regression
- Assumes parameters are fixed and data is random
```{r}
lm_model <- lm(popularity ~ song_age, data = songs)

summary(lm_model)

broom::tidy(lm_model)

confint(lm_model, level = 0.95)

```
# Bayesian linear regression
- Assumes parameters are random and data is fixed
- Interested in range of parameters that could give rise to given data set
```{r}
library(tidyverse)
library(rstanarm)
library(broom.mixed)

songs <- read_csv("data/radiohead_songs.csv")
# songs %>% glimpse()

stan_model <- stan_glm(popularity ~ song_age
                       ,data = songs, )

broom.mixed::tidy(stan_model)

```

# Credible and confidence intervals
- Confidence interval: probability that a range contains the true value
- Credible interval: probability that the true value is within a range
```{r}

posterior_interval(stan_model, prob = 0.89)

```
# Probability that parameters are within a range
```{r}
posterior_dist <- tidybayes::spread_draws(stan_model, song_age) %>% 
    janitor::clean_names()

mean(between(posterior_dist$song_age, 0.001, 0.00179))

```

```{r}
posterior_dist %>% 
    ggplot(aes(x = iteration, y = song_age, color = as.character(chain))) +
    geom_line() +
    theme_minimal() 
```

```{r}
# 3 chains, 1000 iterations, 500 warmup
model_3chains <- stan_glm(popularity ~ song_age, data = songs,
    chains = 3, iter = 1000, warmup = 500)

# Print a summary of model_3chains
summary(model_3chains)

# 2 chains, 100 iterations, 50 warmup
model_2chains <- stan_glm(popularity ~ song_age, data = songs,
    chains = 2, iter = 100, warmup = 50)

# Print a summary of model_2chains
summary(model_2chains)

```

# Addressing errors when fitting
## Divergent transitions
eg: 1: There were 15 divergent transitions after warmup. 
Increasing adapt_delta above 0.8 may help. Default = 0.95
```{r}
# Estimate the model with a new `adapt_delta`
adapt_model <- stan_glm(popularity ~ song_age, data = songs,
  control = list(adapt_delta = 0.99))

# View summary
summary(adapt_model)
```

## Exceeding Maximum Tree Depth
eg: 2: Chain 1 reached the maximum tree depth
Max tree depth indicates poor efficiency
```{r}
# Estimate the model with a new `max_treedepth`
tree_model <- stan_glm(popularity ~ song_age, data = songs,
  control = list(max_treedepth = 15))

# View summary
summary(tree_model)
```

# Assessing model fit for frequentist model
```{r}
# Print the R-squared from the linear model
lm_summary <- summary(lm_model)
lm_summary$r.squared

# Calulate sums of squares
ss_res <- var(residuals(lm_model))
ss_fit <- var(fitted(lm_model))

# Calculate the R-squared
1 - (ss_res / (ss_res + ss_fit))
```
# R-squared for Bayesian Model
How much variance in the predicted variable can be explained by the predictors
```{r}
# Save the variance of residuals
ss_res <- var(residuals(stan_model))

# Save the variance of fitted values
ss_fit <- var(fitted(stan_model))

# Calculate the R-squared
1 - (ss_res / (ss_res + ss_fit))

stan_model$ses
stan_model$fitted.values[1:10]
```
# Posterior predictive model checks
- Compare the distribution of predictions in an iteration to actual distributions
```{r}
# Calculate posterior predictive scores
predictions <- posterior_linpred(stan_model)

summary(songs$popularity)

summary(predictions[1,])

summary(predictions[10,])

```

# Compare individual scores to their expected distribution
```{r}

songs$popularity[12]

summary(predictions[,12])

```
```{r}
# Calculate the posterior distribution of the R-squared
r2_posterior <- bayes_R2(stan_model)

# Make a histogram of the distribution
hist(r2_posterior)
```
```{r}
# Create density comparison
pp_check(stan_model, "dens_overlay")

# Create scatter plot of means and standard deviations
pp_check(stan_model, "stat")
```
# Comparing models
```{r}
model_1pred <- stan_glm(popularity ~ song_age, data = songs)

model_2pred <- stan_glm(popularity ~ song_age + tempo, data = songs)

model_3pred <- stan_glm(popularity ~ song_age + duration_ms, data = songs)

model_4pred <- stan_glm(popularity ~ song_age + time_signature, data = songs)


```  
```{r}
loo_1pred <- loo(model_1pred)

loo_2pred <- loo(model_2pred)

loo_3pred <- loo(model_3pred)

loo_4pred <- loo(model_4pred)

loo_compare(loo_1pred, loo_2pred, loo_3pred, loo_4pred)

# Expected log pointwise predictive density is more negative for worse fitting
# models

# The preferred model is model_1pred
```

# Make predictions with a Bayesian Linear Regression Model
```{r}
library(tidyverse)
library(rstanarm)
library(broom.mixed)

songs <- read_csv("data/radiohead_songs.csv")
set.seed(2023)

# Specified with the same priors as PyStan model - takes longer to run with similar result
# stan_model <- stan_glm(popularity ~ song_age, data = songs, 
#                        prior = normal(0, 100, autoscale = FALSE), 
#                        prior_intercept = normal(0, 100, autoscale = FALSE), 
#                        prior_aux = cauchy(0, 5, autoscale = FALSE))

stan_model <- stan_glm(popularity ~ song_age, data = songs)


# summary(stan_model)
broom.mixed::tidy(stan_model)

# Get posteriors of predicted scores for each observation
posteriors <- posterior_predict(stan_model)

```
# Predict song popularity
```{r}
predict_data <- data.frame(song_age = c(500, 9000))

new_predictions <- posterior_predict(stan_model, newdata = predict_data)

# Predictions for popularity with song age of 500 days
summary(new_predictions[, 1])

# Predictions for popularity with song age of 9000 days
summary(new_predictions[, 2])

```
# Visualise predictions
```{r}
new_predictions <- as.data.frame(new_predictions)
colnames(new_predictions) <- c("500 days", "9000 days")

plot_posterior <- pivot_longer(new_predictions, cols = everything(),
                               names_to = "song_age", 
                               values_to = "predict")

means <- plot_posterior %>%
  group_by(song_age) %>%
  summarise(mean_popularity = mean(predict))

colours <- c("#8B008B", "#BDB76B")

ggplot(plot_posterior, aes(x = predict, fill = song_age)) +
    facet_wrap(~song_age, ncol = 1) +
	geom_density(alpha = 0.8) +
    scale_fill_manual(values = colours) +
    geom_vline(data = means, 
               aes(xintercept = mean_popularity), 
               linetype = "dotted", 
               color = "black") +
    geom_text(data = means, 
              aes(x = mean_popularity, 
                  y = 0, 
                  label = round(mean_popularity, 1)), 
              vjust = -1, 
              size = 4, 
              color = "white") +
    labs(
        title = "Predicted Radiohead song popularity distributions as a function of song age",
        x = "Song popularity on Spotify",
        y = "Density",
        fill = "Song Age"
    ) +
    theme_minimal() +
    theme(legend.position = "None")

```


# Plot best model fit
```{r}
# Save the model parameters
tidy_coef <- tidy(stan_model)

# Extract intercept and slope
model_intercept <- tidy_coef$estimate[1]
model_slope <- tidy_coef$estimate[2]

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
  geom_point() +
  geom_abline(intercept = model_intercept, slope = model_slope)

draws <- tidybayes::spread_draws(stan_model, `(Intercept)`, song_age) 
draws

ggplot(songs, aes(x = song_age, y = popularity)) +
	geom_point(size = 2, alpha = 0.5) +
	geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age),
                size = 0.1, alpha = 0.05, color = "skyblue") +
	geom_abline(intercept = model_intercept, slope = model_slope) +
    labs(title = "Radiohead Song Popularity on Spotify",
         subtitle = "People like the old stuff better than the new",
         caption = "Bayesian linear regression model with uncertainty",
         x = "Song Age",
         y = "Popularity") +
    theme_minimal()

```

## loo_compare example
```{r}
?loo_compare()

LL <- example_loglik_array()
loo1 <- loo(LL, r_eff = NA)     # should be worst model when compared
loo2 <- loo(LL + 1, r_eff = NA) # should be second best model when compared
loo3 <- loo(LL + 2, r_eff = NA) # should be best model when compared

comp <- loo_compare(loo1, loo2, loo3)
print(comp, digits = 2)
```


# Predictions using stan model
```{r}

range(songs$song_age)

library(tidybayes)
library(dplyr)
library(ggdist)

# Define the new data frame with predictor variables for new observations
new_data <- data.frame(song_age = c(400, 1000, 4000, 8000, 10000, 20000))

# Generate predicted values for the response variable (popularity)
pred <- predict(stan_model, newdata = new_data, draws = 100)

# Calculate the mean and 95% credible interval for the predicted values
summary_df <- as_tibble(pred) %>% 
  summarise_all(~mean_qi(.x, probs = c(0.025, 0.975))) %>% 
  unnest(cols = c(value))
summary_df

# Combine the new data and predicted values into a single data frame
plot_data <- bind_cols(new_data, as_tibble(pred))

```


```{python}
# Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools": https://visualstudio.microsoft.com/visual-cpp-build-tools/

# Needed to transfer to WSL Linux version to run successfully on Windows

songs = r.songs

# Define the Stan model
stan_code = """
data {
  int<lower=0> N;
  vector[N] song_age;
  vector[N] popularity;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  vector[N] mu = alpha + beta * song_age;
  
  // Priors
  alpha ~ normal(0, 100);
  beta ~ normal(0, 100);
  sigma ~ cauchy(0, 5);
  
  // Likelihood
  popularity ~ normal(mu, sigma);
}
"""

# Prepare the data for Stan
data = {
    'N': len(songs),
    'song_age': songs['song_age'].values,
    'popularity': songs['popularity'].values
}

# Compile the Stan model
sm = pystan.StanModel(model_code=stan_code)

# Fit the model to the data
fit = sm.sampling(data=data, iter=2000, chains=4, warmup=1000)

# Print the summary
print(fit)

# Convert the results to ArviZ InferenceData
idata = az.from_pystan(fit)

# Extract the parameters and display the summary
params_df = az.summary(idata).reset_index()
params_df.columns = ['parameter', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'mcse', 'ess', 'r_hat']
print(params_df)


```

